<html>
<head>
<title>Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations</title>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='./css/paperstyle.css' rel='stylesheet' type='text/css'>

</head>

<body>

<div class="pageTitle">
    Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations
  <br>
  <br>

  <br>
  <span class = "Authors">
      <a target="_blank">Yushi Du<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a><sup>1*</sup> &nbsp; &nbsp;
      <p style="font-size:12px; "> <i>(*: indicates joint first authors)</i></p>
       <a href="https://sxy7147.github.io/">Yan Zhao</a> <sup>1</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>

      <sup>1</sup> Peking University </a> &nbsp;
  </span>
  </div>
<br>
<br>
<br>
<div class = "material">
        <a target="_blank">[Paper]</a> &nbsp; &nbsp;
        <a target="_blank" href="https://github.com/Yushi-Du/PartMotion">[Code]</a> &nbsp; &nbsp;
        <a target= "_blank" href="@inproceedings{du2023learning,
  title={Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations},
  author={Du, Yushi and Wu, Ruihai and Shen, Yan and Dong, Hao},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2023}}">[BibTex]</a> &nbsp; &nbsp;
</div>

<div class = "abstractTitle">
  Abstract
</div>
<p class = "abstractText">
  Articulated objects (\emph{e.g.}, doors and drawers) exist everywhere in our life.
Different from rigid objects, articulated objects have higher degrees of freedom and are rich in geometries, semantics, and part functions.
Modelling different kinds of parts and articulations with neural networks plays an essential role in articulated object understanding and manipulation, and will further benefit 3D vision and robotics communities.
To model articulated objects, most previous works directly encode articulated objects into a latent space without explicit and interpretable representation. % representations, without specific designs for parts, articulations and part motions.
To provide interpretable representation for articulated object modelling, in this paper, we introduce a novel framework that explicitly disentangles the part motion of articulated objects by predicting the movements of articulated parts.
We utilise spatially continuous neural implicit representations to model the part motion smoothly in the space, and we for the first time achieve few-shot generalisation on novel object categories and different joint motions (\emph{e.g.}, rotation and displacement over different axis).
</p>

<br>
<div class="abstractTitle">
    Video Presentation
</div>
<br>
<center>
<!--  <iframe width="560" height="315" src="https://www.youtube.com/watch?v=aY5-maonUBU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>-->
  <iframe width="560" height="315" src="./img/ID304_video.mp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>



<br>
<br>
<br>

<div class="abstractTitle">
  Articulated Object Representation
</div>
  <img class="bannerImage" src="./img/teaser.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 1.
        There are a plethora of 3D objects around us in the real world.
Compared to those rigid objects with only 6 degrees of freedom (DoF), articulated objects (\emph{e.g.}, doors and drawers) additionally contain semantically and functionally important articulated parts (\emph{e.g.}, the screen of laptops),
resulting in their higher DoFs in state space, and more complicated geometries and functions.
Therefore, understanding and representing articulated objects with diverse geometries and functions is an essential but challenging task for 3D computer vision.  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Our Proposed Framework
</div>
  <img class="bannerImage" src="./img/pipeline.png" ,="" width="600"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 2.
        <b>Overview of our proposed framework</b>
Our proposed framework receives two point clouds from the same articulated object under two different part poses.
      Then generate the object point cloud with a new part pose. It aggregates the geometric information and the pose
      information into a spatially continuous Transformation Grid. During inferencing, conditioned on the new part pose,
      it decodes the transformation of each point by querying each point in the Grid to generate the input object with
      the novel pose.  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Qualitative Results
</div>
  <img class="bannerImage" src="./img/single_result.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 3.
        <b>Qualitative results</b> The results show that our method reserves the most detailed geometries of both
      articulated parts and object bases. For example, our model predicts the straightest door frame and the smoothest
      microwave door surface.
    </p></td></tr></tbody></table>
<!--  <img class="bannerImage" src="./images/fig4.png" ,="" width="800"><br>-->
<!--  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">-->
<!--        Figure 4.-->
<!--        <b>Qualitative results</b> on Breaking Bad dataset for multi-part geometry shape assembly.-->
<!--  </p></td></tr></tbody></table>-->
<!--  <img class="bannerImage" src="./images/fig5.png" ,="" width="800"><br>-->
<!--  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">-->
<!--        Figure 5.-->
<!--        <b>Qualitative results</b> of our method with and without part correlation on Geometric Shape Mating dataset.-->
<!--         We can see the parts match better when considering part correlations in part representations.-->
<!--        </p></td></tr></tbody></table>-->


<!-- <p></p>  -->



<br>
<br>


<br>
<br>


<div class = "abstractTitle">
  Contact
</div>
<p class = "abstractText">
  If you have any questions, please feel free to contact Yushi Du</a> at duyushi628@pku.edu.cn and
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a> at wuruihai@pku.edu.cn.
</p>



<br>
<br>


</body></html>